# -*- coding: utf-8 -*-
"""Classification_whole_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B0RIH-sXSWdTec88A1xiHPqUbJAKffvA
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.models as models
import torch.nn.functional as F
import torch.utils.data as utils
import time

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

device = None
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

file_inputs_P = np.load('/content/drive/My Drive/Graph_Classification/Paldi_features_X.npy')
file_labels_P = np.load('/content/drive/My Drive/Graph_Classification/Paldi_labels_y.npy')

file_inputs_N = np.load('/content/drive/My Drive/Graph_Classification/Nehru_features_X.npy')
file_labels_N = np.load('/content/drive/My Drive/Graph_Classification/Nehru_labels_y.npy')

file_inputs_A = np.load('/content/drive/My Drive/Graph_Classification/APMC_features_X.npy')
file_labels_A = np.load('/content/drive/My Drive/Graph_Classification/APMC_features_y.npy')

def PrepareData(inputs_P, labels_P, inputs_N, labels_N, inputs_A, labels_A, train_proportion = 0.7, valid_proportion = 0.1, batch_size = 20):

  sample_size_P = inputs_P.shape[0]
  train_index_P = int(train_proportion * sample_size_P)
  valid_index_P = int((train_proportion + valid_proportion) * sample_size_P)

  sample_size_N = inputs_N.shape[0]
  train_index_N = int(train_proportion * sample_size_N)
  valid_index_N = int((train_proportion + valid_proportion) * sample_size_N)

  sample_size_A = inputs_A.shape[0]
  train_index_A = int(train_proportion * sample_size_A)
  valid_index_A = int((train_proportion + valid_proportion) * sample_size_A)

  train_inputs, train_labels = inputs_P[:train_index_P], labels_P[:train_index_P]
  train_inputs = np.concatenate((train_inputs, inputs_N[:train_index_N]), axis = 0)
  train_labels = np.concatenate((train_labels, labels_N[:train_index_N]), axis = 0)
  train_inputs = np.concatenate((train_inputs, inputs_A[:train_index_A]), axis = 0)
  train_labels = np.concatenate((train_labels, labels_A[:train_index_A]), axis = 0)

  valid_inputs, valid_labels = inputs_P[train_index_P:valid_index_P], labels_P[train_index_P:valid_index_P]
  valid_inputs = np.concatenate((valid_inputs, inputs_N[train_index_N:valid_index_N]), axis = 0)
  valid_labels = np.concatenate((valid_labels, labels_N[train_index_N:valid_index_N]), axis = 0)
  valid_inputs = np.concatenate((valid_inputs, inputs_A[train_index_A:valid_index_A]), axis = 0)
  valid_labels = np.concatenate((valid_labels, labels_A[train_index_A:valid_index_A]), axis = 0)

  test_inputs, test_labels = inputs_P[valid_index_P:], labels_P[valid_index_P:]
  test_inputs = np.concatenate((test_inputs, inputs_N[valid_index_N:]), axis = 0)
  test_labels = np.concatenate((test_labels, labels_N[valid_index_N:]), axis = 0)
  test_inputs = np.concatenate((test_inputs, inputs_A[valid_index_A:]), axis = 0)
  test_labels = np.concatenate((test_labels, labels_A[valid_index_A:]), axis = 0)

  print(train_inputs.shape, train_labels.shape)
  print(valid_inputs.shape, valid_labels.shape)
  print(test_inputs.shape, test_labels.shape)

  train_dataset = utils.TensorDataset(torch.tensor(train_inputs),torch.tensor(train_labels))
  valid_dataset = utils.TensorDataset(torch.tensor(valid_inputs),torch.tensor(valid_labels))
  test_dataset = utils.TensorDataset(torch.tensor(test_inputs), torch.tensor(test_labels))

  train_dataloader = utils.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = False)
  valid_dataloader = utils.DataLoader(valid_dataset, batch_size = batch_size, shuffle = True, drop_last = False)
  test_dataloader = utils.DataLoader(test_dataset, batch_size = batch_size, shuffle = True, drop_last = False)

  return train_dataloader, valid_dataloader, test_dataloader

train_dataloader, valid_dataloader, test_dataloader = PrepareData(file_inputs_P, file_labels_P, file_inputs_N, file_labels_N, file_inputs_A, file_labels_A)

print(len(train_dataloader))
print(len(valid_dataloader))
print(len(test_dataloader))

input, label = next(iter(train_dataloader))
[batch_size, height, width] = input.size()
print(batch_size)
print(height)
print(width)
print(label.shape)
print(input.dtype)

def check(outputs, labels):
  total = 0
  correct = 0
  for i,j in zip(outputs, labels):
    if i.argmax() == j:
      correct += 1
    total += 1
  return correct/total

def TrainModel( model, train_dataloader, valid_dataloader, learning_rate = 1e-4, num_epochs = 300):

  input, label = next(iter(train_dataloader))
  [batch_size, height, width] = input.size()

  model.to(device)

  loss = torch.nn.CrossEntropyLoss()

  learning_rate = learning_rate
  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

  use_gpu = torch.cuda.is_available()

  cur_time = time.time()
  pre_time = time.time()

  best_model = model
  max_acc = 0

  for epoch in range(num_epochs):

    trained_number = 0

    valid_dataloader_iter = iter(valid_dataloader)

    losses_epochs_train = []
    losses_epochs_valid = []
    accuracy_train = []
    accuracy_valid = []

    for data in train_dataloader:
      inputs, labels = data

      if input.shape[0] != batch_size:
        continue

      inputs, labels = inputs.to(device), labels.to(device)

      model.zero_grad()

      outputs = model(inputs.double())

      loss_train = loss(outputs, labels)
      losses_epochs_train.append(loss_train.data)
      accuracy_train.append(check(outputs, labels))

      optimizer.zero_grad()
      loss_train.backward()
      optimizer.step()

      try:
        inputs_val, labels_val = next(valid_dataloader_iter)
      except StopIteration:
        valid_dataloader_iter = iter(valid_dataloader)
        inputs_val, labels_val = next(valid_dataloader_iter)

      inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)

      outputs_val = model(inputs_val.double())
      loss_valid = loss(outputs_val, labels_val)
      losses_epochs_valid.append(loss_valid.data)
      accuracy_valid.append(check(outputs_val, labels_val))

      trained_number += 1

    avg_losses_epochs_train = sum(losses_epochs_train) / float(len(losses_epochs_train))
    avg_losses_epochs_valid = sum(losses_epochs_valid) / float(len(losses_epochs_valid))
    avg_acc_train = sum(accuracy_train) / float(len(accuracy_train))
    avg_acc_valid = sum(accuracy_valid) / float(len(accuracy_valid))

    cur_time = time.time()
    print(f'Epochs: {epoch}, train_loss: {avg_losses_epochs_train}, train_acc: {avg_acc_train}, valid_loss: {avg_losses_epochs_valid}, valid acc: {avg_acc_valid}, time: {cur_time - pre_time}')
    pre_time = cur_time

    if max_acc < avg_acc_valid:
      max_acc = avg_acc_valid
      best_model = model

  return model, best_model

def TestModel(model, test_dataloader):

  test_acc = []
  test_loss = []
  loss_func = torch.nn.CrossEntropyLoss()
  pred = []
  true = []

  model.eval()

  for input, labels in test_dataloader:

    input, labels = input.to(device), labels.to(device)

    out = model(input.double())

    loss = loss_func(out, labels)
    test_loss.append(loss.data)
    test_acc.append(check(out, labels))

    for x in out:
      pred.append(x.argmax().cpu().data.numpy())
    for x in labels:
      true.append(x.cpu().data.numpy())

  avg_loss = sum(test_loss) / len(test_loss)
  avg_acc = sum(test_acc) / len(test_acc)

  print(f'Test Loss: {avg_loss} Test Accuracy: {avg_acc}')

  return pred, true

import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=50):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerClassifier(nn.Module):

  def __init__(self, nclasses = 3, ndim = 147, nheads = 7, nhid = 512, nlayers = 3, dropout = 0.4):

    super(TransformerClassifier, self).__init__()
    encoder_layer = TransformerEncoderLayer(ndim, nheads, nhid)
    self.encoder = TransformerEncoder(encoder_layer, nlayers)
    self.positional = PositionalEncoding(ndim)
    self.src_mask = None
    self.ndim = ndim
    self.fc = nn.Linear(ndim, nclasses)

  def _generate_src_mask(self, sz):

    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1).to(device)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

  def reduced_sum(self,out):

    size = out.shape
    final_out = torch.zeros(size[0],size[2]).to(device)
    for i in range(size[0]):
      final_out[i,0] = torch.sum(out[i,:,0])
      final_out[i,1] = torch.sum(out[i,:,1])
      final_out[i,2] = torch.sum(out[i,:,2])
    return final_out/size[1]


  def forward(self, input):

    input = input.permute(1,0,2)
    if self.src_mask is None or self.src_mask.size(0) != len(input):
      mask = self._generate_src_mask(len(input))
      self.src_mask = mask

    positonal = self.positional(input)
    out = self.encoder(positonal, self.src_mask)
    out = (out + input)
    out = out.permute(1,0,2)
    out = self.fc(out)
    out = F.softmax(self.reduced_sum(out),dim = 1)
    return out

model = TransformerClassifier().double().to(device)

# PATH = '/content/drive/My Drive/Pre-Trained Weights/IITH-transformer_classifier-90%.pt'
# model.load_state_dict(torch.load(PATH))

model, best = TrainModel(model, train_dataloader, valid_dataloader, num_epochs= 200, learning_rate = 1e-5)

model, best = TrainModel(model, train_dataloader, valid_dataloader, num_epochs= 300, learning_rate = 1e-5)

model, best = TrainModel(model, train_dataloader, valid_dataloader, num_epochs= 300, learning_rate = 1e-4)

model, best = TrainModel(model, train_dataloader, valid_dataloader, num_epochs= 200, learning_rate = 1e-4)

pred, true = TestModel(best, test_dataloader)
from sklearn.metrics import confusion_matrix
c_m = confusion_matrix(true, pred)
df_cm = pd.DataFrame(c_m, index = ['N','C','U'],
                  columns = ['N','C','U'])
sns.set(font_scale=3.0)
plt.figure(figsize = (6,6))
sns.heatmap(df_cm, annot=True, cbar = False, annot_kws={"size": 30}, cmap = 'Greens', linecolor='black', linewidths=0.05)

pred, true = TestModel(model, test_dataloader)

from sklearn.metrics import confusion_matrix
c_m = confusion_matrix(true, pred)
df_cm = pd.DataFrame(c_m, index = ['N','C','U'],
                  columns = ['N','C','U'])
sns.set(font_scale=3.0)
plt.figure(figsize = (6,6))
sns.heatmap(df_cm, annot=True, cbar = False, annot_kws={"size": 30}, cmap = 'Greens', linecolor='black', linewidths=0.05)

torch.save(model.state_dict(),'/content/drive/My Drive/Pre-Trained Weights/IITH-transformer_classifier_true_64.8_W.pt')