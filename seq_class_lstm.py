# -*- coding: utf-8 -*-
"""Seq_class_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_wvYXRGINmM48S5nF7WLONo1YEvGk7fD

Sequence Predictor
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as utils
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import time
from tqdm import tqdm

import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer

device = None
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

file_inputs = np.load('/content/drive/My Drive/Graph_Classification/Paldi_features_X.npy')
file_labels = np.load('/content/drive/My Drive/Graph_Classification/Paldi_labels_y.npy')

print(file_inputs.shape)
print(file_labels.shape)

def TFPrepareDataset(file_inputs, seq_len = 20, pred_len = 10, BATCH_SIZE = 32, train_ratio = 0.7, valid_ratio = 0.1):

  input_sequence = []
  input_target_sequence = []
  target_sequence = []
  for input in file_inputs:
    input_len = input.shape[0]
    for i in range(input_len - seq_len - pred_len):
      input_sequence.append(input[i : i + seq_len, : ])
      input_target_sequence.append(input[i + seq_len - 1 : i + seq_len + pred_len - 1])
      target_sequence.append(input[i + seq_len : i + seq_len + pred_len])
  input_sequence = np.asarray(input_sequence)
  input_target_sequence = np.asarray(input_target_sequence)
  target_sequence = np.asarray(target_sequence)

  print("Input Sequence Shape: ", input_sequence.shape)
  print("Input Target Sequence Shape: ", input_target_sequence.shape)
  print("Target Sequence Shape: ", target_sequence.shape)

  train_index = int(np.floor(len(input_sequence) * train_ratio))
  valid_index =  int(np.floor(len(input_sequence)* (train_ratio + valid_ratio)))

  train_input, train_target = torch.tensor(input_sequence[:train_index]), torch.tensor(target_sequence[:train_index])
  train_input_target = torch.tensor(input_target_sequence[:train_index])
  valid_input, valid_target = torch.tensor(input_sequence[train_index:valid_index]), torch.tensor(target_sequence[train_index:valid_index])
  valid_input_target = torch.tensor(input_target_sequence[train_index:valid_index])
  test_input, test_target = torch.tensor(input_sequence[valid_index:]), torch.tensor(target_sequence[valid_index:])
  test_input_target = torch.tensor(input_target_sequence[valid_index:])

  train_dataset = utils.TensorDataset(train_input, train_input_target, train_target)
  valid_dataset = utils.TensorDataset(valid_input, valid_input_target, valid_target)
  test_dataset = utils.TensorDataset(test_input, test_input_target, test_target)

  train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)
  test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last= True)

  return train_dataloader, valid_dataloader, test_dataloader

train_dataloader, valid_dataloader, test_dataloader = TFPrepareDataset(file_inputs)

input, input_labels, labels = next(iter(train_dataloader))
[batch_size, seq_len, fea_size] = input.size()
[batch_size, pred_len, fea_size] = labels.size()
print("Batch Size: ", batch_size)
print("Seq len: ", seq_len)
print("Prediction len: ", pred_len)
print("Input Target Seq Size: ", input_labels.shape)

print(len(train_dataloader))
print(len(valid_dataloader))
print(len(test_dataloader))

def TFTrain(model, train_dataloader, valid_dataloader, epochs = 10, learning_rate = 1e-4, valid_force = True):

  model = model.to(device)
  optimizer = optim.RMSprop(model.parameters(), learning_rate)
  loss_func = nn.MSELoss()

  curr = time.time()
  prev = time.time()

  for epoch in range(epochs):
    train_loss = []
    valid_loss = []
    for data in train_dataloader:
      input, input_labels, labels = data
      input, input_labels, labels = input.to(device), input_labels.to(device), labels.to(device)

      optimizer.zero_grad()
      model.zero_grad()

      out = model(input.double(), input_labels.double())
      loss_train = loss_func(out, labels)
      train_loss.append(loss_train.data)

      loss_train.backward()
      optimizer.step()

    for data in valid_dataloader:
      val_input, val_input_labels, val_labels = data
      val_input, val_input_labels, val_labels = val_input.to(device), val_input_labels.to(device), val_labels.to(device)

      if valid_force == True:
        val_out = model(val_input.double(), val_input_labels)
      else:
        val_out = model(val_input.double(), val_input_labels[:,0:1,:].double(), 0, pred_len)

      val_loss = loss_func(val_out, val_labels)
      valid_loss.append(val_loss.data)


    avg_train_loss = sum(train_loss) / len(train_loss)
    avg_valid_loss = sum(valid_loss) / len(valid_loss)
    curr = time.time()

    print(f'Epochs: {epoch}  Train Loss: {avg_train_loss}  Validation Loss: {avg_valid_loss} Time: {curr-prev}')
    prev = curr
  return model

def TFTest(model, test_dataloader, test_force = True):

  loss_func_mse = nn.MSELoss()
  loss_func_l1 = nn.L1Loss()

  test_loss_mse = []
  test_loss_l1 = []
  for data in test_dataloader:

    input, input_labels, labels = data
    input, input_labels, labels = input.to(device), input_labels.to(device), labels.to(device)

    if test_force == True:
      out = model(input.double(), input_labels.double())
    else:
      out = model(input.double(), input_labels[:,0:1,:].double(),0,pred_len)

    loss_mse = loss_func_mse(out, labels)
    loss_l1 = loss_func_l1(out, labels)

    test_loss_mse.append(loss_mse.data)
    test_loss_l1.append(loss_l1.data)

  avg_test_loss_mse = sum(test_loss_mse) / len(test_loss_mse)
  avg_test_loss_l1 = sum(test_loss_l1) / len(test_loss_l1)

  print(f'Test MSE Loss: {avg_test_loss_mse}  Test L1 Loss: {avg_test_loss_l1}')

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerEncoderDecoder(nn.Module):

  def __init__(self, d_model = 147, nheads = 3, num_layers = 2):

    super(TransformerEncoderDecoder, self).__init__()
    encoder_layer = nn.TransformerEncoderLayer(d_model, nheads)
    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
    self.postional_encode = PositionalEncoding(d_model)
    decoder_layer = nn.TransformerDecoderLayer(d_model, nheads)
    self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
    self.encoder_mask = None
    self.decoder_mask = None
    self.d_model = d_model

  def generate_mask(self, src_len):
    mask = (torch.triu(torch.ones(src_len,src_len)) == 1).transpose(0,1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

  def forward(self, input, target, is_train = 1, pred_length = 2):

    input = input.permute(1, 0, 2)
    target = target.permute(1, 0, 2)

    if self.encoder_mask is None or self.encoder_mask.size(0) != len(input):
      mask = self.generate_mask(len(input)).to(device)
      self.encoder_mask = mask

    if self.decoder_mask is None or self.decoder_mask.size(0) != len(target):
      mask = self.generate_mask(len(target)).to(device)
      self.decoder_mask = mask

    src = self.postional_encode(input)
    encoder_out = self.encoder(src, self.encoder_mask)

    if is_train == 1:
      decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
      return decoder_out.permute(1,0,2)

    else:
      outputs = None
      for i in range(pred_length):
        decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
        if outputs is None:
            outputs = decoder_out
        else:
            outputs = torch.cat((outputs, decoder_out), 0)
        target = decoder_out
      return outputs.permute(1,0,2)

trans_net = TransformerEncoderDecoder().double().to(device)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, learning_rate = 1e-3, epochs = 50)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, learning_rate = 1e-5, epochs = 50)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, epochs = 10, learning_rate = 1e-6, valid_force = False)

TFTest(trans_net, test_dataloader, test_force = True)

TFTest(trans_net, test_dataloader, test_force = False)

torch.save(trans_net.state_dict(),'/content/drive/My Drive/Pre-Trained Weights/IITH_generator_P_0055.pt')

"""Seq + Class"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as utils
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import time
import torchvision.models as models

import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer

device = None
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

file_inputs = np.load('/content/drive/My Drive/Graph_Classification/Paldi_features_X.npy')
file_labels = np.load('/content/drive/My Drive/Graph_Classification/Paldi_labels_y.npy')

print(file_inputs.shape)
print(file_labels.shape)

def TFPrepareDataset(file_inputs, file_labels, seq_len = 20, pred_len = 10, BATCH_SIZE = 32, train_ratio = 0.7, valid_ratio = 0.1):

  input_sequence = []
  input_target_sequence = []
  target_sequence = []
  labels = []

  for idx, input in enumerate(file_inputs):
    input_len = input.shape[0]
    for i in range(input_len - seq_len - pred_len):
      input_sequence.append(input[i : i + seq_len, : ])
      input_target_sequence.append(input[i + seq_len - 1 : i + seq_len + pred_len - 1])
      target_sequence.append(input[i + seq_len : i + seq_len + pred_len])
      labels.append(file_labels[idx])

  input_sequence = np.asarray(input_sequence)
  input_target_sequence = np.asarray(input_target_sequence)
  target_sequence = np.asarray(target_sequence)
  labels = np.asarray(labels)

  print("Input Sequence Shape: ", input_sequence.shape)
  print("Input Target Sequence Shape: ", input_target_sequence.shape)
  print("Target Sequence Shape: ", target_sequence.shape)
  print("labels Shape: ", labels.shape)

  train_index = int(np.floor(len(input_sequence) * train_ratio))
  valid_index = int(np.floor(len(input_sequence) * (train_ratio + valid_ratio)))

  train_input, train_target = torch.tensor(input_sequence[:train_index]), torch.tensor(target_sequence[:train_index])
  train_input_target = torch.tensor(input_target_sequence[:train_index])
  train_labels = torch.tensor(labels[:train_index])

  valid_input, valid_target = torch.tensor(input_sequence[train_index:valid_index]), torch.tensor(target_sequence[train_index:valid_index])
  valid_input_target = torch.tensor(input_target_sequence[train_index:valid_index])
  valid_labels = torch.tensor(labels[train_index:valid_index])

  test_input, test_target = torch.tensor(input_sequence[valid_index:]), torch.tensor(target_sequence[valid_index:])
  test_input_target = torch.tensor(input_target_sequence[valid_index:])
  test_labels = torch.tensor(labels[valid_index:])

  train_dataset = utils.TensorDataset(train_input, train_input_target, train_target, train_labels)
  valid_dataset = utils.TensorDataset(valid_input, valid_input_target, valid_target, valid_labels)
  test_dataset = utils.TensorDataset(test_input, test_input_target, test_target, test_labels)

  train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  valid_dataloder = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last= True)

  return train_dataloader, valid_dataloder, test_dataloader

train_dataloader, valid_dataloader, test_dataloader = TFPrepareDataset(file_inputs, file_labels)

input, input_target_seq, target_seq, labels = next(iter(train_dataloader))
[batch_size, seq_len, fea_size] = input.size()
[batch_size, pred_len, fea_size] = target_seq.size()
print("Batch Size: ", batch_size)
print("Seq len: ", seq_len)
print("Prediction len: ", pred_len)
print("Input Target Seq Size: ", input_target_seq.shape)
print("Labels Size: ", labels.shape)

print(len(train_dataloader))
print(len(valid_dataloader))
print(len(test_dataloader))

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerEncoderDecoder(nn.Module):

  def __init__(self, d_model = 147, nheads = 3, num_layers = 2):

    super(TransformerEncoderDecoder, self).__init__()
    encoder_layer = nn.TransformerEncoderLayer(d_model, nheads)
    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
    self.postional_encode = PositionalEncoding(d_model)
    decoder_layer = nn.TransformerDecoderLayer(d_model, nheads)
    self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
    self.encoder_mask = None
    self.decoder_mask = None
    self.d_model = d_model

  def generate_mask(self, src_len):
    mask = (torch.triu(torch.ones(src_len,src_len)) == 1).transpose(0,1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

  def forward(self, input, target, is_train = 1, pred_length = 2):

    input = input.permute(1, 0, 2)
    target = target.permute(1, 0, 2)

    if self.encoder_mask is None or self.encoder_mask.size(0) != len(input):
      mask = self.generate_mask(len(input)).to(device)
      self.encoder_mask = mask

    if self.decoder_mask is None or self.decoder_mask.size(0) != len(target):
      mask = self.generate_mask(len(target)).to(device)
      self.decoder_mask = mask

    src = self.postional_encode(input)
    encoder_out = self.encoder(src, self.encoder_mask)

    if is_train == 1:
      decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
      return decoder_out.permute(1,0,2)

    else:
      outputs = None
      for i in range(pred_length):
        decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
        if outputs is None:
            outputs = decoder_out
        else:
            outputs = torch.cat((outputs, decoder_out), 0)
        target = decoder_out
      return outputs.permute(1,0,2)

trans_net = TransformerEncoderDecoder().double().to(device)

PATH = '/content/drive/My Drive/Pre-Trained Weights/IITH_generator_P_0055.pt'
trans_net.load_state_dict(torch.load(PATH))
trans_net.eval()

def final_outputs(model, dataloader, valid_force = True):

  final_outs = []
  final_labels = []

  model.eval()

  for idx, data in enumerate(dataloader):

    input, input_target_seq, target_seq, labels = data
    input, input_target_seq = input.to(device), input_target_seq.to(device)

    if valid_force == True:
      out1 = model(input.double(), input_target_seq.double())
    else:
      out1 = model(input.double(),input_target_seq[:,0:1,:].double(), 0, pred_len)
    out = out1.to(torch.device('cpu')).clone().detach()

    final_outs.append(out)
    final_labels.append(labels)

  return final_outs, final_labels

train_outs, train_labels = final_outputs(trans_net, train_dataloader, valid_force=True)
valid_outs, valid_labels = final_outputs(trans_net, valid_dataloader, valid_force=True)
test_outs, test_labels = final_outputs(trans_net, test_dataloader, valid_force=True)

print(len(train_outs), len(train_labels))
print(len(valid_outs), len(valid_labels))
print(len(test_outs), len(test_labels))

def check(outputs, labels):
  total = 0
  correct = 0
  for i,j in zip(outputs, labels):
    if i.argmax() == j:
      correct += 1
    total += 1
  return correct/total

def FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels, lr = 1e-4, epochs = 20):


  optimizer = optim.Adam(model.parameters(), lr = lr)
  loss_func = nn.CrossEntropyLoss()

  cur_time = time.time()
  pre_time = time.time()

  for epoch in range(epochs):

    train_loss = []
    train_acc = []
    valid_loss = []
    valid_acc = []

    for (input, labels) in zip(train_outs, train_labels):

      input, labels = input.to(device), labels.to(device)

      optimizer.zero_grad()
      model.zero_grad()

      out = model(input.double())
      loss = loss_func(out, labels)

      train_loss.append(loss.data)
      train_acc.append(check(out,labels))

      loss.backward()
      optimizer.step()

    for (input, labels) in zip(valid_outs, valid_labels):

      input, labels = input.to(device), labels.to(device)

      out = model(input.double())
      loss = loss_func(out, labels)

      valid_loss.append(loss.data)
      valid_acc.append(check(out, labels))

    cur_time = time.time()
    avg_train_loss = sum(train_loss) / len(train_loss)
    avg_train_acc = sum(train_acc) / len(train_acc)
    avg_valid_loss = sum(valid_loss) / len(valid_loss)
    avg_valid_acc = sum(valid_acc) / len(valid_acc)

    print(f'Epochs: {epoch} Train Loss: {avg_train_loss} Train Acc: {avg_train_acc} Test Loss: {avg_valid_loss} Test Acc: {avg_valid_acc} Time: {cur_time - pre_time}')
    pre_time = cur_time

  return model

def FinalTest(model, test_outs, test_labels):

  loss_func = nn.CrossEntropyLoss()
  test_loss = []
  test_acc = []

  for (input, labels) in zip(test_outs, test_labels):

    input, labels = input.to(device), labels.to(device)

    out = model(input.double())
    loss = loss_func(out, labels)

    test_loss.append(loss.data)
    test_acc.append(check(out, labels))

  avg_loss = sum(test_loss) / len(test_loss)
  avg_acc = sum(test_acc) / len(test_acc)

  print(f'Test Loss: {avg_loss}  Test Accuracy: {avg_acc}')

class Classifier(nn.Module):

  def __init__(self, d_model = 147, n_classes = 3):
    super(Classifier, self).__init__()
    self.gru_1 = nn.GRU(d_model, 100, 1,batch_first = True)
    self.gru_2 = nn.GRU(100,50,1,batch_first = True)
    self.fc = nn.Linear(50,n_classes)

  def forward(self, input):

    out, _ = self.gru_1(input)
    _, h_c = self.gru_2(out)
    h_c = h_c[0]
    h_c = F.softmax(self.fc(h_c),dim = 1)

    return h_c

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=50):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerClassifier(nn.Module):

  def __init__(self, nclasses = 3, ndim = 147, nheads = 7, nhid = 512, nlayers = 3, dropout = 0.4):

    super(TransformerClassifier, self).__init__()
    encoder_layer = TransformerEncoderLayer(ndim, nheads, nhid)
    self.encoder = TransformerEncoder(encoder_layer, nlayers)
    self.positional = PositionalEncoding(ndim)
    self.src_mask = None
    self.ndim = ndim
    self.fc = nn.Linear(ndim, nclasses)

  def _generate_src_mask(self, sz):

    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1).to(device)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

  def reduced_sum(self,out):

    size = out.shape
    final_out = torch.zeros(size[0],size[2]).to(device)
    for i in range(size[0]):
      final_out[i,0] = torch.sum(out[i,:,0])
      final_out[i,1] = torch.sum(out[i,:,1])
      final_out[i,2] = torch.sum(out[i,:,2])
    return final_out/size[1]


  def forward(self, input):

    input = input.permute(1,0,2)
    if self.src_mask is None or self.src_mask.size(0) != len(input):
      mask = self._generate_src_mask(len(input))
      self.src_mask = mask

    positonal = self.positional(input)
    out = self.encoder(positonal, self.src_mask)
    out = (out + input)
    out = out.permute(1,0,2)
    out = self.fc(out)
    out = F.softmax(self.reduced_sum(out),dim = 1)
    return out

model = TransformerClassifier().double().to(device)
# PATH = '/content/drive/My Drive/Pre-Trained Weights/IITH_Final_58_P.pt'
# model.load_state_dict(torch.load(PATH))

model = FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels,lr = 1e-5, epochs = 20)

model = FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels,lr = 1e-5, epochs = 100)

FinalTest(model, test_outs, test_labels)

torch.save(model.state_dict(),'/content/drive/My Drive/Pre-Trained Weights/IITH_Final_58_P.pt')

"""Nehru - Dataset"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as utils
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import time
from tqdm import tqdm

import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer

device = None
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

file_inputs = np.load('/content/drive/My Drive/Graph_Classification/Nehru_features_X.npy')
file_labels = np.load('/content/drive/My Drive/Graph_Classification/Nehru_labels_y.npy')

print(file_inputs.shape)
print(file_labels.shape)

def TFPrepareDataset(file_inputs, seq_len = 20, pred_len = 10, BATCH_SIZE = 32, train_ratio = 0.7, valid_ratio = 0.1):

  input_sequence = []
  input_target_sequence = []
  target_sequence = []
  for input in file_inputs:
    input_len = input.shape[0]
    for i in range(input_len - seq_len - pred_len):
      input_sequence.append(input[i : i + seq_len, : ])
      input_target_sequence.append(input[i + seq_len - 1 : i + seq_len + pred_len - 1])
      target_sequence.append(input[i + seq_len : i + seq_len + pred_len])
  input_sequence = np.asarray(input_sequence)
  input_target_sequence = np.asarray(input_target_sequence)
  target_sequence = np.asarray(target_sequence)

  print("Input Sequence Shape: ", input_sequence.shape)
  print("Input Target Sequence Shape: ", input_target_sequence.shape)
  print("Target Sequence Shape: ", target_sequence.shape)

  train_index = int(np.floor(len(input_sequence) * train_ratio))
  valid_index =  int(np.floor(len(input_sequence)* (train_ratio + valid_ratio)))

  train_input, train_target = torch.tensor(input_sequence[:train_index]), torch.tensor(target_sequence[:train_index])
  train_input_target = torch.tensor(input_target_sequence[:train_index])
  valid_input, valid_target = torch.tensor(input_sequence[train_index:valid_index]), torch.tensor(target_sequence[train_index:valid_index])
  valid_input_target = torch.tensor(input_target_sequence[train_index:valid_index])
  test_input, test_target = torch.tensor(input_sequence[valid_index:]), torch.tensor(target_sequence[valid_index:])
  test_input_target = torch.tensor(input_target_sequence[valid_index:])

  train_dataset = utils.TensorDataset(train_input, train_input_target, train_target)
  valid_dataset = utils.TensorDataset(valid_input, valid_input_target, valid_target)
  test_dataset = utils.TensorDataset(test_input, test_input_target, test_target)

  train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)
  test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last= True)

  return train_dataloader, valid_dataloader, test_dataloader

train_dataloader, valid_dataloader, test_dataloader = TFPrepareDataset(file_inputs)

input, input_labels, labels = next(iter(train_dataloader))
[batch_size, seq_len, fea_size] = input.size()
[batch_size, pred_len, fea_size] = labels.size()
print("Batch Size: ", batch_size)
print("Seq len: ", seq_len)
print("Prediction len: ", pred_len)
print("Input Target Seq Size: ", input_labels.shape)

print(len(train_dataloader))
print(len(valid_dataloader))
print(len(test_dataloader))

def TFTrain(model, train_dataloader, valid_dataloader, epochs = 10, learning_rate = 1e-4, valid_force = True):

  model = model.to(device)
  optimizer = optim.RMSprop(model.parameters(), learning_rate)
  loss_func = nn.MSELoss()

  curr = time.time()
  prev = time.time()

  for epoch in range(epochs):
    train_loss = []
    valid_loss = []
    for data in train_dataloader:
      input, input_labels, labels = data
      input, input_labels, labels = input.to(device), input_labels.to(device), labels.to(device)

      optimizer.zero_grad()
      model.zero_grad()

      out = model(input.double(), input_labels.double())
      loss_train = loss_func(out, labels)
      train_loss.append(loss_train.data)

      loss_train.backward()
      optimizer.step()

    for data in valid_dataloader:
      val_input, val_input_labels, val_labels = data
      val_input, val_input_labels, val_labels = val_input.to(device), val_input_labels.to(device), val_labels.to(device)

      if valid_force == True:
        val_out = model(val_input.double(), val_input_labels)
      else:
        val_out = model(val_input.double(), val_input_labels[:,0:1,:].double(), 0, pred_len)

      val_loss = loss_func(val_out, val_labels)
      valid_loss.append(val_loss.data)


    avg_train_loss = sum(train_loss) / len(train_loss)
    avg_valid_loss = sum(valid_loss) / len(valid_loss)
    curr = time.time()

    print(f'Epochs: {epoch}  Train Loss: {avg_train_loss}  Validation Loss: {avg_valid_loss} Time: {curr-prev}')
    prev = curr
  return model

def TFTest(model, test_dataloader, test_force = True):

  loss_func_mse = nn.MSELoss()
  loss_func_l1 = nn.L1Loss()

  test_loss_mse = []
  test_loss_l1 = []
  for data in test_dataloader:

    input, input_labels, labels = data
    input, input_labels, labels = input.to(device), input_labels.to(device), labels.to(device)

    if test_force == True:
      out = model(input.double(), input_labels.double())
    else:
      out = model(input.double(), input_labels[:,0:1,:].double(),0,pred_len)

    loss_mse = loss_func_mse(out, labels)
    loss_l1 = loss_func_l1(out, labels)

    test_loss_mse.append(loss_mse.data)
    test_loss_l1.append(loss_l1.data)

  avg_test_loss_mse = sum(test_loss_mse) / len(test_loss_mse)
  avg_test_loss_l1 = sum(test_loss_l1) / len(test_loss_l1)

  print(f'Test MSE Loss: {avg_test_loss_mse}  Test L1 Loss: {avg_test_loss_l1}')

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerEncoderDecoder(nn.Module):

  def __init__(self, d_model = 147, nheads = 3, num_layers = 2):

    super(TransformerEncoderDecoder, self).__init__()
    encoder_layer = nn.TransformerEncoderLayer(d_model, nheads)
    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
    self.postional_encode = PositionalEncoding(d_model)
    decoder_layer = nn.TransformerDecoderLayer(d_model, nheads)
    self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
    self.encoder_mask = None
    self.decoder_mask = None
    self.d_model = d_model

  def generate_mask(self, src_len):
    mask = (torch.triu(torch.ones(src_len,src_len)) == 1).transpose(0,1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

  def forward(self, input, target, is_train = 1, pred_length = 2):

    input = input.permute(1, 0, 2)
    target = target.permute(1, 0, 2)

    if self.encoder_mask is None or self.encoder_mask.size(0) != len(input):
      mask = self.generate_mask(len(input)).to(device)
      self.encoder_mask = mask

    if self.decoder_mask is None or self.decoder_mask.size(0) != len(target):
      mask = self.generate_mask(len(target)).to(device)
      self.decoder_mask = mask

    src = self.postional_encode(input)
    encoder_out = self.encoder(src, self.encoder_mask)

    if is_train == 1:
      decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
      return decoder_out.permute(1,0,2)

    else:
      outputs = None
      for i in range(pred_length):
        decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
        if outputs is None:
            outputs = decoder_out
        else:
            outputs = torch.cat((outputs, decoder_out), 0)
        target = decoder_out
      return outputs.permute(1,0,2)

trans_net = TransformerEncoderDecoder().double().to(device)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, learning_rate = 1e-3, epochs = 50)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, learning_rate = 1e-5, epochs = 50)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, epochs = 10, learning_rate = 1e-6, valid_force = False)

TFTest(trans_net, test_dataloader, test_force = True)

TFTest(trans_net, test_dataloader, test_force = False)

torch.save(trans_net.state_dict(),'/content/drive/My Drive/Pre-Trained Weights/IITH_generator_N_0119.pt')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as utils
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import time
import torchvision.models as models

import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer

device = None
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

file_inputs = np.load('/content/drive/My Drive/Graph_Classification/Nehru_features_X.npy')
file_labels = np.load('/content/drive/My Drive/Graph_Classification/Nehru_labels_y.npy')

print(file_inputs.shape)
print(file_labels.shape)

def TFPrepareDataset(file_inputs, file_labels, seq_len = 20, pred_len = 10, BATCH_SIZE = 32, train_ratio = 0.7, valid_ratio = 0.1):

  input_sequence = []
  input_target_sequence = []
  target_sequence = []
  labels = []

  for idx, input in enumerate(file_inputs):
    input_len = input.shape[0]
    for i in range(input_len - seq_len - pred_len):
      input_sequence.append(input[i : i + seq_len, : ])
      input_target_sequence.append(input[i + seq_len - 1 : i + seq_len + pred_len - 1])
      target_sequence.append(input[i + seq_len : i + seq_len + pred_len])
      labels.append(file_labels[idx])

  input_sequence = np.asarray(input_sequence)
  input_target_sequence = np.asarray(input_target_sequence)
  target_sequence = np.asarray(target_sequence)
  labels = np.asarray(labels)

  print("Input Sequence Shape: ", input_sequence.shape)
  print("Input Target Sequence Shape: ", input_target_sequence.shape)
  print("Target Sequence Shape: ", target_sequence.shape)
  print("labels Shape: ", labels.shape)

  train_index = int(np.floor(len(input_sequence) * train_ratio))
  valid_index = int(np.floor(len(input_sequence) * (train_ratio + valid_ratio)))

  train_input, train_target = torch.tensor(input_sequence[:train_index]), torch.tensor(target_sequence[:train_index])
  train_input_target = torch.tensor(input_target_sequence[:train_index])
  train_labels = torch.tensor(labels[:train_index])

  valid_input, valid_target = torch.tensor(input_sequence[train_index:valid_index]), torch.tensor(target_sequence[train_index:valid_index])
  valid_input_target = torch.tensor(input_target_sequence[train_index:valid_index])
  valid_labels = torch.tensor(labels[train_index:valid_index])

  test_input, test_target = torch.tensor(input_sequence[valid_index:]), torch.tensor(target_sequence[valid_index:])
  test_input_target = torch.tensor(input_target_sequence[valid_index:])
  test_labels = torch.tensor(labels[valid_index:])

  train_dataset = utils.TensorDataset(train_input, train_input_target, train_target, train_labels)
  valid_dataset = utils.TensorDataset(valid_input, valid_input_target, valid_target, valid_labels)
  test_dataset = utils.TensorDataset(test_input, test_input_target, test_target, test_labels)

  train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  valid_dataloder = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last= True)

  return train_dataloader, valid_dataloder, test_dataloader

train_dataloader, valid_dataloader, test_dataloader = TFPrepareDataset(file_inputs, file_labels)

input, input_target_seq, target_seq, labels = next(iter(train_dataloader))
[batch_size, seq_len, fea_size] = input.size()
[batch_size, pred_len, fea_size] = target_seq.size()
print("Batch Size: ", batch_size)
print("Seq len: ", seq_len)
print("Prediction len: ", pred_len)
print("Input Target Seq Size: ", input_target_seq.shape)
print("Labels Size: ", labels.shape)

print(len(train_dataloader))
print(len(valid_dataloader))
print(len(test_dataloader))

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerEncoderDecoder(nn.Module):

  def __init__(self, d_model = 147, nheads = 3, num_layers = 2):

    super(TransformerEncoderDecoder, self).__init__()
    encoder_layer = nn.TransformerEncoderLayer(d_model, nheads)
    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
    self.postional_encode = PositionalEncoding(d_model)
    decoder_layer = nn.TransformerDecoderLayer(d_model, nheads)
    self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
    self.encoder_mask = None
    self.decoder_mask = None
    self.d_model = d_model

  def generate_mask(self, src_len):
    mask = (torch.triu(torch.ones(src_len,src_len)) == 1).transpose(0,1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

  def forward(self, input, target, is_train = 1, pred_length = 2):

    input = input.permute(1, 0, 2)
    target = target.permute(1, 0, 2)

    if self.encoder_mask is None or self.encoder_mask.size(0) != len(input):
      mask = self.generate_mask(len(input)).to(device)
      self.encoder_mask = mask

    if self.decoder_mask is None or self.decoder_mask.size(0) != len(target):
      mask = self.generate_mask(len(target)).to(device)
      self.decoder_mask = mask

    src = self.postional_encode(input)
    encoder_out = self.encoder(src, self.encoder_mask)

    if is_train == 1:
      decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
      return decoder_out.permute(1,0,2)

    else:
      outputs = None
      for i in range(pred_length):
        decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
        if outputs is None:
            outputs = decoder_out
        else:
            outputs = torch.cat((outputs, decoder_out), 0)
        target = decoder_out
      return outputs.permute(1,0,2)

trans_net = TransformerEncoderDecoder().double().to(device)

PATH = '/content/drive/My Drive/Pre-Trained Weights/IITH_generator_N_0119.pt'
trans_net.load_state_dict(torch.load(PATH))
trans_net.eval()

def final_outputs(model, dataloader, valid_force = True):

  final_outs = []
  final_labels = []

  model.eval()

  for idx, data in enumerate(dataloader):

    input, input_target_seq, target_seq, labels = data
    input, input_target_seq = input.to(device), input_target_seq.to(device)

    if valid_force == True:
      out1 = model(input.double(), input_target_seq.double())
    else:
      out1 = model(input.double(),input_target_seq[:,0:1,:].double(), 0, pred_len)
    out = out1.to(torch.device('cpu')).clone().detach()

    final_outs.append(out)
    final_labels.append(labels)

  return final_outs, final_labels

train_outs, train_labels = final_outputs(trans_net, train_dataloader, valid_force=True)
valid_outs, valid_labels = final_outputs(trans_net, valid_dataloader, valid_force=True)
test_outs, test_labels = final_outputs(trans_net, test_dataloader, valid_force=True)

print(len(train_outs), len(train_labels))
print(len(valid_outs), len(valid_labels))
print(len(test_outs), len(test_labels))

def check(outputs, labels):
  total = 0
  correct = 0
  for i,j in zip(outputs, labels):
    if i.argmax() == j:
      correct += 1
    total += 1
  return correct/total

def FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels, lr = 1e-4, epochs = 20):


  optimizer = optim.Adam(model.parameters(), lr = lr)
  loss_func = nn.CrossEntropyLoss()

  cur_time = time.time()
  pre_time = time.time()

  for epoch in range(epochs):

    train_loss = []
    train_acc = []
    valid_loss = []
    valid_acc = []

    for (input, labels) in zip(train_outs, train_labels):

      input, labels = input.to(device), labels.to(device)

      optimizer.zero_grad()
      model.zero_grad()

      out = model(input.double())
      loss = loss_func(out, labels)

      train_loss.append(loss.data)
      train_acc.append(check(out,labels))

      loss.backward()
      optimizer.step()

    for (input, labels) in zip(valid_outs, valid_labels):

      input, labels = input.to(device), labels.to(device)

      out = model(input.double())
      loss = loss_func(out, labels)

      valid_loss.append(loss.data)
      valid_acc.append(check(out, labels))

    cur_time = time.time()
    avg_train_loss = sum(train_loss) / len(train_loss)
    avg_train_acc = sum(train_acc) / len(train_acc)
    avg_valid_loss = sum(valid_loss) / len(valid_loss)
    avg_valid_acc = sum(valid_acc) / len(valid_acc)

    print(f'Epochs: {epoch} Train Loss: {avg_train_loss} Train Acc: {avg_train_acc} Test Loss: {avg_valid_loss} Test Acc: {avg_valid_acc} Time: {cur_time - pre_time}')
    pre_time = cur_time

  return model

def FinalTest(model, test_outs, test_labels):

  loss_func = nn.CrossEntropyLoss()
  test_loss = []
  test_acc = []

  for (input, labels) in zip(test_outs, test_labels):

    input, labels = input.to(device), labels.to(device)

    out = model(input.double())
    loss = loss_func(out, labels)

    test_loss.append(loss.data)
    test_acc.append(check(out, labels))

  avg_loss = sum(test_loss) / len(test_loss)
  avg_acc = sum(test_acc) / len(test_acc)

  print(f'Test Loss: {avg_loss}  Test Accuracy: {avg_acc}')

class Classifier(nn.Module):

  def __init__(self, d_model = 147, n_classes = 3):
    super(Classifier, self).__init__()
    self.gru_1 = nn.GRU(d_model, 100, 1,batch_first = True)
    self.gru_2 = nn.GRU(100,50,1,batch_first = True)
    self.fc = nn.Linear(50,n_classes)

  def forward(self, input):

    out, _ = self.gru_1(input)
    _, h_c = self.gru_2(out)
    h_c = h_c[0]
    h_c = F.softmax(self.fc(h_c),dim = 1)

    return h_c

model = Classifier().double().to(device)
# PATH = '/content/drive/My Drive/Pre-Trained Weights/IITH_Final_76_N.pt'
# model.load_state_dict(torch.load(PATH))

model = FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels,lr = 1e-4, epochs = 20)

model = FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels,lr = 1e-4, epochs = 50)

FinalTest(model, test_outs, test_labels)

torch.save(model.state_dict(),'/content/drive/My Drive/Pre-Trained Weights/IITH_Final_76_N.pt')

"""APMC"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as utils
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import time
from tqdm import tqdm

import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer

device = None
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

file_inputs = np.load('/content/drive/My Drive/Graph_Classification/APMC_features_X.npy')
file_labels = np.load('/content/drive/My Drive/Graph_Classification/APMC_features_y.npy')

print(file_inputs.shape)
print(file_labels.shape)

def TFPrepareDataset(file_inputs, seq_len = 20, pred_len = 10, BATCH_SIZE = 32, train_ratio = 0.7, valid_ratio = 0.1):

  input_sequence = []
  input_target_sequence = []
  target_sequence = []
  for input in file_inputs:
    input_len = input.shape[0]
    for i in range(input_len - seq_len - pred_len):
      input_sequence.append(input[i : i + seq_len, : ])
      input_target_sequence.append(input[i + seq_len - 1 : i + seq_len + pred_len - 1])
      target_sequence.append(input[i + seq_len : i + seq_len + pred_len])
  input_sequence = np.asarray(input_sequence)
  input_target_sequence = np.asarray(input_target_sequence)
  target_sequence = np.asarray(target_sequence)

  print("Input Sequence Shape: ", input_sequence.shape)
  print("Input Target Sequence Shape: ", input_target_sequence.shape)
  print("Target Sequence Shape: ", target_sequence.shape)

  train_index = int(np.floor(len(input_sequence) * train_ratio))
  valid_index =  int(np.floor(len(input_sequence)* (train_ratio + valid_ratio)))

  train_input, train_target = torch.tensor(input_sequence[:train_index]), torch.tensor(target_sequence[:train_index])
  train_input_target = torch.tensor(input_target_sequence[:train_index])
  valid_input, valid_target = torch.tensor(input_sequence[train_index:valid_index]), torch.tensor(target_sequence[train_index:valid_index])
  valid_input_target = torch.tensor(input_target_sequence[train_index:valid_index])
  test_input, test_target = torch.tensor(input_sequence[valid_index:]), torch.tensor(target_sequence[valid_index:])
  test_input_target = torch.tensor(input_target_sequence[valid_index:])

  train_dataset = utils.TensorDataset(train_input, train_input_target, train_target)
  valid_dataset = utils.TensorDataset(valid_input, valid_input_target, valid_target)
  test_dataset = utils.TensorDataset(test_input, test_input_target, test_target)

  train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)
  test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last= True)

  return train_dataloader, valid_dataloader, test_dataloader

train_dataloader, valid_dataloader, test_dataloader = TFPrepareDataset(file_inputs)

input, input_labels, labels = next(iter(train_dataloader))
[batch_size, seq_len, fea_size] = input.size()
[batch_size, pred_len, fea_size] = labels.size()
print("Batch Size: ", batch_size)
print("Seq len: ", seq_len)
print("Prediction len: ", pred_len)
print("Input Target Seq Size: ", input_labels.shape)

print(len(train_dataloader))
print(len(valid_dataloader))
print(len(test_dataloader))

def TFTrain(model, train_dataloader, valid_dataloader, epochs = 10, learning_rate = 1e-4, valid_force = True):

  model = model.to(device)
  optimizer = optim.RMSprop(model.parameters(), learning_rate)
  loss_func = nn.MSELoss()

  curr = time.time()
  prev = time.time()

  for epoch in range(epochs):
    train_loss = []
    valid_loss = []
    for data in train_dataloader:
      input, input_labels, labels = data
      input, input_labels, labels = input.to(device), input_labels.to(device), labels.to(device)

      optimizer.zero_grad()
      model.zero_grad()

      out = model(input.double(), input_labels.double())
      loss_train = loss_func(out, labels.double())
      train_loss.append(loss_train.data)

      loss_train.backward()
      optimizer.step()

    for data in valid_dataloader:
      val_input, val_input_labels, val_labels = data
      val_input, val_input_labels, val_labels = val_input.to(device), val_input_labels.to(device), val_labels.to(device)

      if valid_force == True:
        val_out = model(val_input.double(), val_input_labels.double())
      else:
        val_out = model(val_input.double(), val_input_labels[:,0:1,:].double(), 0, pred_len)

      val_loss = loss_func(val_out, val_labels.double())
      valid_loss.append(val_loss.data)


    avg_train_loss = sum(train_loss) / len(train_loss)
    avg_valid_loss = sum(valid_loss) / len(valid_loss)
    curr = time.time()

    print(f'Epochs: {epoch}  Train Loss: {avg_train_loss}  Validation Loss: {avg_valid_loss} Time: {curr-prev}')
    prev = curr
  return model

def TFTest(model, test_dataloader, test_force = True):

  loss_func_mse = nn.MSELoss()
  loss_func_l1 = nn.L1Loss()

  test_loss_mse = []
  test_loss_l1 = []
  for data in test_dataloader:

    input, input_labels, labels = data
    input, input_labels, labels = input.to(device), input_labels.to(device), labels.to(device)

    if test_force == True:
      out = model(input.double(), input_labels.double())
    else:
      out = model(input.double(), input_labels[:,0:1,:].double(),0,pred_len)

    loss_mse = loss_func_mse(out, labels.double())
    loss_l1 = loss_func_l1(out, labels.double())

    test_loss_mse.append(loss_mse.data)
    test_loss_l1.append(loss_l1.data)

  avg_test_loss_mse = sum(test_loss_mse) / len(test_loss_mse)
  avg_test_loss_l1 = sum(test_loss_l1) / len(test_loss_l1)

  print(f'Test MSE Loss: {avg_test_loss_mse}  Test L1 Loss: {avg_test_loss_l1}')

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerEncoderDecoder(nn.Module):

  def __init__(self, d_model = 147, nheads = 3, num_layers = 2):

    super(TransformerEncoderDecoder, self).__init__()
    encoder_layer = nn.TransformerEncoderLayer(d_model, nheads)
    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
    self.postional_encode = PositionalEncoding(d_model)
    decoder_layer = nn.TransformerDecoderLayer(d_model, nheads)
    self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
    self.encoder_mask = None
    self.decoder_mask = None
    self.d_model = d_model

  def generate_mask(self, src_len):
    mask = (torch.triu(torch.ones(src_len,src_len)) == 1).transpose(0,1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

  def forward(self, input, target, is_train = 1, pred_length = 2):

    input = input.permute(1, 0, 2)
    target = target.permute(1, 0, 2)

    if self.encoder_mask is None or self.encoder_mask.size(0) != len(input):
      mask = self.generate_mask(len(input)).to(device)
      self.encoder_mask = mask

    if self.decoder_mask is None or self.decoder_mask.size(0) != len(target):
      mask = self.generate_mask(len(target)).to(device)
      self.decoder_mask = mask

    src = self.postional_encode(input)
    encoder_out = self.encoder(src, self.encoder_mask)

    if is_train == 1:
      decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
      return decoder_out.permute(1,0,2)

    else:
      outputs = None
      for i in range(pred_length):
        decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
        if outputs is None:
            outputs = decoder_out
        else:
            outputs = torch.cat((outputs, decoder_out), 0)
        target = decoder_out
      return outputs.permute(1,0,2)

trans_net = TransformerEncoderDecoder().double().to(device)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, learning_rate = 1e-3, epochs = 50)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, learning_rate = 1e-5, epochs = 50)

trans_net = TFTrain(trans_net, train_dataloader, valid_dataloader, epochs = 10, learning_rate = 1e-6, valid_force = False)

TFTest(trans_net, test_dataloader, test_force = True)

TFTest(trans_net, test_dataloader, test_force = False)

torch.save(trans_net.state_dict(),'/content/drive/My Drive/Pre-Trained Weights/IITH_generator_A_0018.pt')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as utils
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import time
import torchvision.models as models

import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer

device = None
if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

file_inputs = np.load('/content/drive/My Drive/Graph_Classification/APMC_features_X.npy')
file_labels = np.load('/content/drive/My Drive/Graph_Classification/APMC_features_y.npy')

print(file_inputs.shape)
print(file_labels.shape)

def TFPrepareDataset(file_inputs, file_labels, seq_len = 20, pred_len = 10, BATCH_SIZE = 32, train_ratio = 0.7, valid_ratio = 0.1):

  input_sequence = []
  input_target_sequence = []
  target_sequence = []
  labels = []

  for idx, input in enumerate(file_inputs):
    input_len = input.shape[0]
    for i in range(input_len - seq_len - pred_len):
      input_sequence.append(input[i : i + seq_len, : ])
      input_target_sequence.append(input[i + seq_len - 1 : i + seq_len + pred_len - 1])
      target_sequence.append(input[i + seq_len : i + seq_len + pred_len])
      labels.append(file_labels[idx])

  input_sequence = np.asarray(input_sequence)
  input_target_sequence = np.asarray(input_target_sequence)
  target_sequence = np.asarray(target_sequence)
  labels = np.asarray(labels)

  print("Input Sequence Shape: ", input_sequence.shape)
  print("Input Target Sequence Shape: ", input_target_sequence.shape)
  print("Target Sequence Shape: ", target_sequence.shape)
  print("labels Shape: ", labels.shape)

  train_index = int(np.floor(len(input_sequence) * train_ratio))
  valid_index = int(np.floor(len(input_sequence) * (train_ratio + valid_ratio)))

  train_input, train_target = torch.tensor(input_sequence[:train_index]), torch.tensor(target_sequence[:train_index])
  train_input_target = torch.tensor(input_target_sequence[:train_index])
  train_labels = torch.tensor(labels[:train_index])

  valid_input, valid_target = torch.tensor(input_sequence[train_index:valid_index]), torch.tensor(target_sequence[train_index:valid_index])
  valid_input_target = torch.tensor(input_target_sequence[train_index:valid_index])
  valid_labels = torch.tensor(labels[train_index:valid_index])

  test_input, test_target = torch.tensor(input_sequence[valid_index:]), torch.tensor(target_sequence[valid_index:])
  test_input_target = torch.tensor(input_target_sequence[valid_index:])
  test_labels = torch.tensor(labels[valid_index:])

  train_dataset = utils.TensorDataset(train_input, train_input_target, train_target, train_labels)
  valid_dataset = utils.TensorDataset(valid_input, valid_input_target, valid_target, valid_labels)
  test_dataset = utils.TensorDataset(test_input, test_input_target, test_target, test_labels)

  train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  valid_dataloder = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)
  test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last= True)

  return train_dataloader, valid_dataloder, test_dataloader

train_dataloader, valid_dataloader, test_dataloader = TFPrepareDataset(file_inputs, file_labels)

input, input_target_seq, target_seq, labels = next(iter(train_dataloader))
[batch_size, seq_len, fea_size] = input.size()
[batch_size, pred_len, fea_size] = target_seq.size()
print("Batch Size: ", batch_size)
print("Seq len: ", seq_len)
print("Prediction len: ", pred_len)
print("Input Target Seq Size: ", input_target_seq.shape)
print("Labels Size: ", labels.shape)

print(len(train_dataloader))
print(len(valid_dataloader))
print(len(test_dataloader))

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerEncoderDecoder(nn.Module):

  def __init__(self, d_model = 147, nheads = 3, num_layers = 2):

    super(TransformerEncoderDecoder, self).__init__()
    encoder_layer = nn.TransformerEncoderLayer(d_model, nheads)
    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
    self.postional_encode = PositionalEncoding(d_model)
    decoder_layer = nn.TransformerDecoderLayer(d_model, nheads)
    self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
    self.encoder_mask = None
    self.decoder_mask = None
    self.d_model = d_model

  def generate_mask(self, src_len):
    mask = (torch.triu(torch.ones(src_len,src_len)) == 1).transpose(0,1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

  def forward(self, input, target, is_train = 1, pred_length = 2):

    input = input.permute(1, 0, 2)
    target = target.permute(1, 0, 2)

    if self.encoder_mask is None or self.encoder_mask.size(0) != len(input):
      mask = self.generate_mask(len(input)).to(device)
      self.encoder_mask = mask

    if self.decoder_mask is None or self.decoder_mask.size(0) != len(target):
      mask = self.generate_mask(len(target)).to(device)
      self.decoder_mask = mask

    src = self.postional_encode(input)
    encoder_out = self.encoder(src, self.encoder_mask)

    if is_train == 1:
      decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
      return decoder_out.permute(1,0,2)

    else:
      outputs = None
      for i in range(pred_length):
        decoder_out = self.decoder(target, encoder_out, self.decoder_mask)
        if outputs is None:
            outputs = decoder_out
        else:
            outputs = torch.cat((outputs, decoder_out), 0)
        target = decoder_out
      return outputs.permute(1,0,2)

trans_net = TransformerEncoderDecoder().double().to(device)

PATH = '/content/drive/My Drive/Pre-Trained Weights/IITH_generator_A_0018.pt'
trans_net.load_state_dict(torch.load(PATH))
trans_net.eval()

def final_outputs(model, dataloader, valid_force = True):

  final_outs = []
  final_labels = []

  model.eval()

  for idx, data in enumerate(dataloader):

    input, input_target_seq, target_seq, labels = data
    input, input_target_seq = input.to(device), input_target_seq.to(device)

    if valid_force == True:
      out1 = model(input.double(), input_target_seq.double())
    else:
      out1 = model(input.double(),input_target_seq[:,0:1,:].double(), 0, pred_len)
    out = out1.to(torch.device('cpu')).clone().detach()

    final_outs.append(out)
    final_labels.append(labels)

  return final_outs, final_labels

train_outs, train_labels = final_outputs(trans_net, train_dataloader, valid_force=True)
valid_outs, valid_labels = final_outputs(trans_net, valid_dataloader, valid_force=True)
test_outs, test_labels = final_outputs(trans_net, test_dataloader, valid_force=True)

test_outs, test_labels = final_outputs(trans_net, test_dataloader, valid_force=False)

print(len(train_outs), len(train_labels))
print(len(valid_outs), len(valid_labels))
print(len(test_outs), len(test_labels))

def check(outputs, labels):
  total = 0
  correct = 0
  for i,j in zip(outputs, labels):
    if i.argmax() == j:
      correct += 1
    total += 1
  return correct/total

def FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels, lr = 1e-4, epochs = 20):


  optimizer = optim.Adam(model.parameters(), lr = lr)
  loss_func = nn.CrossEntropyLoss()

  cur_time = time.time()
  pre_time = time.time()

  for epoch in range(epochs):

    train_loss = []
    train_acc = []
    valid_loss = []
    valid_acc = []

    for (input, labels) in zip(train_outs, train_labels):

      input, labels = input.to(device), labels.to(device)

      optimizer.zero_grad()
      model.zero_grad()

      out = model(input.double())
      loss = loss_func(out, labels)

      train_loss.append(loss.data)
      train_acc.append(check(out,labels))

      loss.backward()
      optimizer.step()

    for (input, labels) in zip(valid_outs, valid_labels):

      input, labels = input.to(device), labels.to(device)

      out = model(input.double())
      loss = loss_func(out, labels)

      valid_loss.append(loss.data)
      valid_acc.append(check(out, labels))

    cur_time = time.time()
    avg_train_loss = sum(train_loss) / len(train_loss)
    avg_train_acc = sum(train_acc) / len(train_acc)
    avg_valid_loss = sum(valid_loss) / len(valid_loss)
    avg_valid_acc = sum(valid_acc) / len(valid_acc)

    print(f'Epochs: {epoch} Train Loss: {avg_train_loss} Train Acc: {avg_train_acc} Test Loss: {avg_valid_loss} Test Acc: {avg_valid_acc} Time: {cur_time - pre_time}')
    pre_time = cur_time

  return model

def FinalTest(model, test_outs, test_labels):

  loss_func = nn.CrossEntropyLoss()
  test_loss = []
  test_acc = []

  for (input, labels) in zip(test_outs, test_labels):

    input, labels = input.to(device), labels.to(device)

    out = model(input.double())
    loss = loss_func(out, labels)

    test_loss.append(loss.data)
    test_acc.append(check(out, labels))

  avg_loss = sum(test_loss) / len(test_loss)
  avg_acc = sum(test_acc) / len(test_acc)

  print(f'Test Loss: {avg_loss}  Test Accuracy: {avg_acc}')

class Classifier(nn.Module):

  def __init__(self, d_model = 147, n_classes = 3):
    super(Classifier, self).__init__()
    self.gru_1 = nn.GRU(d_model, 100, 1,batch_first = True)
    self.gru_2 = nn.GRU(100,50,1,batch_first = True)
    self.fc = nn.Linear(50,n_classes)

  def forward(self, input):

    out, _ = self.gru_1(input)
    _, h_c = self.gru_2(out)
    h_c = h_c[0]
    h_c = F.softmax(self.fc(h_c),dim = 1)

    return h_c

model = Classifier().double().to(device)
PATH = '/content/drive/My Drive/Pre-Trained Weights/IITH_Final_45_AL.pt'
model.load_state_dict(torch.load(PATH))

model = FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels, lr = 1e-4, epochs = 20)

model = FinalTrain(model, train_outs, train_labels, valid_outs, valid_labels,lr = 1e-4, epochs = 50)

FinalTest(model, test_outs, test_labels)

torch.save(model.state_dict(),'/content/drive/My Drive/Pre-Trained Weights/IITH_Final_45_AL.pt')